# Furssah-AI-Competition--Tunisia-


# ğŸ” Cybersecurity RAG Assistant

A powerful Retrieval-Augmented Generation (RAG) assistant tailored for cybersecurity researchers, penetration testers, and threat analysts. This assistant ingests documents and web content (PDFs, DOCX, Markdown, and URLs), embeds and indexes them, and answers questions with contextually accurate responses using local or cloud-hosted LLMs.

---

## ğŸš€ Features

- ğŸ§  **RAG Pipeline**: Combine document retrieval and remote LLMs for accurate answers.  
- ğŸ“„ **Multi-Format Ingestion**: Supports `.pdf`, `.docx`, `.md`, and URLs.  
- ğŸŒ **Web & GitHub Integration**: Ingest cybersecurity content from websites and GitHub repos.  
- âš™ï¸ **Modular Design**: Easily extendable pipelines (feature, training, inference).  
- ğŸ§© **Embeddings + ChromaDB**: Semantic vector search with sentence-transformers.  
- ğŸ¤– **Remote LLM Support**: Uses Colab-hosted LLMs or local models like TinyLlama.  
- ğŸ›¡ï¸ **Cybersecurity-Focused**: Optimized prompts, sources, and pipeline for pentesting, CVEs, and security analysis.  
- ğŸ“¦ **FastAPI Backend**: Clean API for `/ingest` and `/ask`.

---

## ğŸ“ Project Structure

<pre>
cybersec-rag/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ server/
â”‚   â”‚   â””â”€â”€ main.py             # FastAPI app
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py     # Main RAG logic
â”‚   â”‚   â”œâ”€â”€ prompt_engine.py    # Prompt formatting (customizable)
â”‚   â”‚   â””â”€â”€ llm_colab_client.py # Remote LLM communication
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ ingestion.py        # File & URL ingestion
â”‚   â”‚   â””â”€â”€ utils.py            # Helpers for parsing & cleaning
â”‚   â””â”€â”€ schema/
â”‚       â””â”€â”€ schema.py           # Pydantic schemas
â”œâ”€â”€ vector_store/               # ChromaDB storage
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
</pre>

---

## ğŸ§ª Quickstart

### 1. Clone the Repo

```bash
git clone https://github.com/cemek7/cybersec-RAG-bot.git
cd cybersec-rag
```

### 2. Install Requirements

```bash
pip install -r requirements.txt
```

### 3. Run the Backend

```bash
uvicorn app.server.main:app --reload
```

Visit [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs) to explore the API via Swagger UI.

---

## ğŸ“¤ Endpoints

### POST /ingest

Upload files or URLs for ingestion and embedding.

```bash
curl -X POST http://localhost:8000/ingest \\
  -F "file=@sample_report.pdf"
```

### POST /ask

Ask questions about ingested content.

Example payload:

```json
{
  "query": "What is CVE-2023-24055 and how is it exploited?"
}
```

---

## ğŸ§  LLM Support

- TinyLlama 1.1B (CPU) â€“ for low-latency refinement locally  
- Remote LLMs via Colab (e.g., Mistral/Mixtral) â€“ for main generation

Configure your cloud LLM endpoint inside `llm_colab_client.py`.

---

## ğŸ› ï¸ Customization

- ğŸ”§ Plug in your own embedding models or vector DBs  
- ğŸ”Œ Add new sources (e.g., RSS feeds, CVE feeds, GitHub orgs)  
- ğŸ§ª Customize prompts in `prompt_engine.py`  
- ğŸ” Enable metadata filters for domain-specific answers

---

## ğŸ“¦ Dependencies

- `fastapi`, `uvicorn`  
- `langchain`, `sentence-transformers`  
- `chromadb`  
- `PyMuPDF`, `python-docx`, `markdown`, `beautifulsoup4`  
- `llama-cpp-python` *(for local inference)*  
- `aiohttp`, `pydantic`, `tqdm`

---

## âœ… Roadmap

- [x] File + URL ingestion  
- [x] Local + remote LLM support  
- [x] Modular RAG pipeline  
- [ ] GitHub repo auto-ingestion  
- [ ] Streamlit-based frontend UI  
- [ ] API key and auth middleware  
- [ ] Auto-CVE feed integration from NVD, Shodan, ExploitDB

---

## ğŸ‘¨â€ğŸ’» Maintainer

**christopher ozougwu** â€“ [yourwebsite.com](https://yourwebsite.com)  
Built with â¤ï¸ for security researchers, red teamers, and AI hobbyists.

---

## ğŸ“„ License

MIT License. See the `LICENSE` file for full details.
